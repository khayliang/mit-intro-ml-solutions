{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 Part 2\n",
    "Provided code for homework below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provided code for Processing Auto Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing code_for_hw03 (part 2, imported as hw3)\n",
      "Imported tidy_plot, plot_separator, plot_data, plot_nonlin_sep, cv, rv, y, positive, score\n",
      "         xval_learning_alg, eval_classifier\n",
      "Tests: test_linear_classifier\n",
      "Dataset tools: load_auto_data, std_vals, standard, raw, one_hot, auto_data_and_labels\n",
      "               load_review_data, clean, extract_words, bag_of_words, extract_bow_feature_vectors\n",
      "               load_mnist_data, load_mnist_single\n",
      "avg and std {}\n",
      "entries in one_hot field {}\n",
      "auto data and labels shape (6, 392) (1, 392)\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import code_for_hw3_part2 as hw3\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Auto Data\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Returns a list of dictionaries.  Keys are the column names, including mpg.\n",
    "auto_data_all = hw3.load_auto_data('auto-mpg.tsv')\n",
    "\n",
    "# The choice of feature processing for each feature, mpg is always raw and\n",
    "# does not need to be specified.  Other choices are hw3.standard and hw3.one_hot.\n",
    "# 'name' is not numeric and would need a different encoding.\n",
    "features = [('cylinders', hw3.raw),\n",
    "            ('displacement', hw3.raw),\n",
    "            ('horsepower', hw3.raw),\n",
    "            ('weight', hw3.raw),\n",
    "            ('acceleration', hw3.raw),\n",
    "            ## Drop model_year by default\n",
    "            ## ('model_year', hw3.raw),\n",
    "            ('origin', hw3.raw)]\n",
    "\n",
    "# Construct the standard data and label arrays\n",
    "auto_data, auto_labels = hw3.auto_data_and_labels(auto_data_all, features)\n",
    "print('auto data and labels shape', auto_data.shape, auto_labels.shape)\n",
    "\n",
    "if False:                               # set to True to see histograms\n",
    "    import matplotlib.pyplot as plt\n",
    "    for feat in range(auto_data.shape[0]):\n",
    "        print('Feature', feat, features[feat][0])\n",
    "        # Plot histograms in one window, different colors\n",
    "        plt.hist(auto_data[feat,auto_labels[0,:] > 0])\n",
    "        plt.hist(auto_data[feat,auto_labels[0,:] < 0])\n",
    "        plt.show()\n",
    "        # Plot histograms in two windows, different colors\n",
    "        fig,(a1,a2) = plt.subplots(nrows=2)\n",
    "        a1.hist(auto_data[feat,auto_labels[0,:] > 0])\n",
    "        a2.hist(auto_data[feat,auto_labels[0,:] < 0])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Auto Data\n",
    "My code for processing auto data\n",
    "\n",
    "We know of two algorithm classes: perceptron and averaged perceptron (which we implemented in HW 1). We have a several parameters that specify the settings for these learning algorithms.\n",
    "\n",
    "A) Which parameters should we use for the learning algorithm? In the perceptron and averaged perceptron, there is a single parameter, \n",
    "T\n",
    "T, the number of iterations.\n",
    "\n",
    "B) Which features should we use? We have lots of choices here: we can use any subset of the data columns and for each column we have choices of how to compute features.\n",
    "\n",
    "C) We will use expected accuracy, estimated by 10-fold cross-validation (we have included the definition in the code file), to make these choices of parameters.\n",
    "\n",
    "We will try two types of algorithms: perceptron and averaged perceptron.\n",
    "\n",
    "We will try 3 values of $T: T=1, T=10, T=50$.\n",
    "\n",
    "We will try 2 feature sets:\n",
    "\n",
    "```[cylinders=raw, displacement=raw, horsepower=raw, weight=raw, acceleration=raw, origin=raw]```\n",
    "\n",
    "```[cylinders=one_hot, displacement=standard, horsepower=standard, weight=standard, acceleration=standard, origin=one_hot]```\n",
    "\n",
    "Perform 10-fold cross-validation for all combinations of the two algorithms, three \n",
    "T\n",
    "T values, and the two choices of feature sets. It will be worthwhile investing in a piece of code to carry out all of the evaluations, in case you need to do this more than once.\n",
    "\n",
    "In general, you should shuffle the dataset before evaluating, but for this exercise, please use hw3.xval_learning_alg, which shuffles the dataset for you, so that your results match ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg and std {'displacement': (388.3482142857143, 302.0458123396403), 'horsepower': (509.3545918367347, 333.6521151716361), 'weight': (2977.5841836734694, 848.3184465698365), 'acceleration': (15.541326530612228, 2.7553429127509963)}\n",
      "entries in one_hot field {'cylinders': [3.0, 4.0, 5.0, 6.0, 8.0], 'origin': [1.0, 2.0, 3.0]}\n",
      "raw auto data and labels shape (6, 392) (1, 392)\n",
      "processed auto data and labels shape (12, 392) (1, 392)\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# Analyze auto data\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "features_processed = [('cylinders', hw3.one_hot),\n",
    "            ('displacement', hw3.standard),\n",
    "            ('horsepower', hw3.standard),\n",
    "            ('weight', hw3.standard),\n",
    "            ('acceleration', hw3.standard),\n",
    "            ## Drop model_year by default\n",
    "            ## ('model_year', hw3.raw),\n",
    "            ('origin', hw3.one_hot)]\n",
    "\n",
    "perceptron_1 = lambda data, labels: hw3.perceptron(data, labels, params={'T': 1})\n",
    "averaged_perceptron_1 = lambda data, labels: hw3.averaged_perceptron(data, labels, params={'T': 1})\n",
    "\n",
    "perceptron_10 = lambda data, labels: hw3.perceptron(data, labels, params={'T': 10})\n",
    "averaged_perceptron_10 = lambda data, labels: hw3.averaged_perceptron(data, labels, params={'T': 10})\n",
    "\n",
    "perceptron_50 = lambda data, labels: hw3.perceptron(data, labels, params={'T': 50})\n",
    "averaged_perceptron_50 = lambda data, labels: hw3.averaged_perceptron(data, labels, params={'T': 50})\n",
    "\n",
    "# process feature_set_1\n",
    "auto_data_raw = auto_data\n",
    "auto_labels_raw = auto_labels\n",
    "auto_data_processed, auto_labels_processed = hw3.auto_data_and_labels(auto_data_all, features_processed)\n",
    "\n",
    "print('raw auto data and labels shape', auto_data_raw.shape, auto_labels_raw.shape)\n",
    "print('processed auto data and labels shape', auto_data_processed.shape, auto_labels_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal, T: 1, raw:  0.6526282051282052\n",
      "averaged, T: 1, raw:  0.8441025641025641\n",
      "normal, T: 10, raw:  0.7423076923076924\n",
      "averaged, T: 10, raw:  0.8366025641025641\n",
      "normal, T: 50, raw:  0.6909615384615384\n",
      "averaged, T: 50, raw:  0.8366025641025641\n"
     ]
    }
   ],
   "source": [
    "# Cross validation on raw data\n",
    "print(\"normal, T: 1, raw: \", hw3.xval_learning_alg(perceptron_1, auto_data_raw, auto_labels_raw, 10))\n",
    "print(\"averaged, T: 1, raw: \", hw3.xval_learning_alg(averaged_perceptron_1, auto_data_raw, auto_labels_raw, 10))\n",
    "print(\"normal, T: 10, raw: \", hw3.xval_learning_alg(perceptron_10, auto_data_raw, auto_labels_raw, 10))\n",
    "print(\"averaged, T: 10, raw: \", hw3.xval_learning_alg(averaged_perceptron_10, auto_data_raw, auto_labels_raw, 10))\n",
    "print(\"normal, T: 50, raw: \", hw3.xval_learning_alg(perceptron_50, auto_data_raw, auto_labels_raw, 10))\n",
    "print(\"averaged, T: 50, raw: \", hw3.xval_learning_alg(averaged_perceptron_50, auto_data_raw, auto_labels_raw, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal, T: 1, processed:  0.7908333333333333\n",
      "averaged, T: 1, processed:  0.9004487179487182\n",
      "normal, T: 10, processed:  0.8061538461538461\n",
      "averaged, T: 10, processed:  0.8979487179487181\n",
      "normal, T: 50, processed:  0.8060256410256409\n",
      "averaged, T: 50, processed:  0.9005128205128207\n"
     ]
    }
   ],
   "source": [
    "# Cross validation on processed data\n",
    "print(\"normal, T: 1, processed: \", hw3.xval_learning_alg(perceptron_1, auto_data_processed, auto_labels_processed, 10))\n",
    "print(\"averaged, T: 1, processed: \", hw3.xval_learning_alg(averaged_perceptron_1, auto_data_processed, auto_labels_processed, 10))\n",
    "print(\"normal, T: 10, processed: \", hw3.xval_learning_alg(perceptron_10, auto_data_processed, auto_labels_processed, 10))\n",
    "print(\"averaged, T: 10, processed: \", hw3.xval_learning_alg(averaged_perceptron_10, auto_data_processed, auto_labels_processed, 10))\n",
    "print(\"normal, T: 50, processed: \", hw3.xval_learning_alg(perceptron_50, auto_data_processed, auto_labels_processed, 10))\n",
    "print(\"averaged, T: 50, processed: \", hw3.xval_learning_alg(averaged_perceptron_50, auto_data_processed, auto_labels_processed, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.92168367]\n",
      " [ 1.50280612]\n",
      " [ 0.40229592]\n",
      " [-0.7630102 ]\n",
      " [ 0.73903061]\n",
      " [-1.17833746]\n",
      " [ 0.45228459]\n",
      " [-4.91225636]\n",
      " [ 0.45783691]\n",
      " [ 0.27806122]\n",
      " [ 0.97091837]\n",
      " [-0.28954082]]\n"
     ]
    }
   ],
   "source": [
    "# finding weights of averaged perceptron with T: 10, processed dataset\n",
    "th, th0 = averaged_perceptron_10(auto_data_processed, auto_labels_processed)\n",
    "print(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Sentiment of Review Data\n",
    "Code block below is provided code for homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_bow_data and labels shape (19945, 10000) (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# Review Data\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Returns lists of dictionaries.  Keys are the column names, 'sentiment' and 'text'.\n",
    "# The train data has 10,000 examples\n",
    "review_data = hw3.load_review_data('reviews.tsv')\n",
    "\n",
    "# Lists texts of reviews and list of labels (1 or -1)\n",
    "review_texts, review_label_list = zip(*((sample['text'], sample['sentiment']) for sample in review_data))\n",
    "\n",
    "# The dictionary of all the words for \"bag of words\"\n",
    "dictionary = hw3.bag_of_words(review_texts)\n",
    "\n",
    "# The standard data arrays for the bag of words\n",
    "review_bow_data = hw3.extract_bow_feature_vectors(review_texts, dictionary)\n",
    "review_labels = hw3.rv(review_label_list)\n",
    "print('review_bow_data and labels shape', review_bow_data.shape, review_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Review Data\n",
    "We have two algorithm classes: perceptron and averaged perceptron. We have a couple of choices of parameters to make to completely specify the learning algorithms.\n",
    "\n",
    "5.1A) Which parameters should we use for the learning algorithm? In the perceptron and averaged perceptron, there is a single parameter, $T$, the number of iterations.\n",
    "\n",
    "5.1B) Which features should we use? We could do variations of bag-of-words, for example, simply indicating whether a word is present or, alternatively, using a count of how many times it is present. We can also remove commonly used words with little information; the code distribution includes a file of those words: stopwords.txt. You're welcome to explore these on your own; we'll use only a binary indicator for all the words.\n",
    "\n",
    "5.1C) Perform 10-fold cross-validation for all combinations of the two algorithms and three $T$ values (1, 10, 50). Record the accuracies for each combination (there should be 6 values total).\n",
    "\n",
    "Note: These tests may take a couple of minutes to run; don't expect instant response, but it shouldn't run for 10 minutes.\n",
    "\n",
    "Now we have the data we need to make rational choices.\n",
    "\n",
    "5.1D) Which algorithm class is typically more effective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# Analyze review data\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Cross validation on bag of words data\n",
    "# print(\"normal, T: 1: \", hw3.xval_learning_alg(perceptron_1, review_bow_data, review_labels, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"averaged, T: 1: \", hw3.xval_learning_alg(averaged_perceptron_1, review_bow_data, review_labels, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"normal, T: 10: \", hw3.xval_learning_alg(perceptron_10, review_bow_data, review_labels, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"averaged, T: 10: \", hw3.xval_learning_alg(averaged_perceptron_10, review_bow_data, review_labels, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"normal, T: 50: \", hw3.xval_learning_alg(perceptron_50, review_bow_data, review_labels, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"averaged, T: 50: \", hw3.xval_learning_alg(averaged_perceptron_50, review_bow_data, review_labels, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_data_all loaded. shape of single images is (28, 28)\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# MNIST Data\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Returns a dictionary formatted as follows:\n",
    "{\n",
    "    0: {\n",
    "        \"images\": [(m by n image), (m by n image), ...],\n",
    "        \"labels\": [0, 0, ..., 0]\n",
    "    },\n",
    "    1: {...},\n",
    "    ...\n",
    "    9\n",
    "}\n",
    "Where labels range from 0 to 9 and (m, n) images are represented\n",
    "by arrays of floats from 0 to 1\n",
    "\"\"\"\n",
    "mnist_data_all = hw3.load_mnist_data(range(10))\n",
    "\n",
    "print('mnist_data_all loaded. shape of single images is', mnist_data_all[0][\"images\"][0].shape)\n",
    "\n",
    "def raw_mnist_features(x):\n",
    "    \"\"\"\n",
    "    @param x (n_samples,m,n) array with values in (0,1)\n",
    "    @return (m*n,n_samples) reshaped array where each entry is preserved\n",
    "    \"\"\"\n",
    "    samples_amt, m, n = x.shape\n",
    "    return np.reshape(x, (samples_amt, m*n)).T\n",
    "\n",
    "\n",
    "def row_average_features(x):\n",
    "    \"\"\"\n",
    "    This should either use or modify your code from the tutor questions.\n",
    "\n",
    "    @param x (n_samples,m,n) array with values in (0,1)\n",
    "    @return (m,n_samples) array where each entry is the average of a row\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "    \n",
    "    def extract_feature(img):\n",
    "        \"\"\"\n",
    "        @param img (m,n) array with values in (0,1)\n",
    "        @return (m,1) array where each entry is the average of a row\n",
    "        \"\"\"\n",
    "        m, n = img.shape\n",
    "        return np.sum(img, axis=1)/n\n",
    "    \n",
    "    for img in x:\n",
    "        arr.append(extract_feature(img))\n",
    "\n",
    "    return np.array(arr).T\n",
    "\n",
    "def col_average_features(x):\n",
    "    \"\"\"\n",
    "    This should either use or modify your code from the tutor questions.\n",
    "\n",
    "    @param x (n_samples,m,n) array with values in (0,1)\n",
    "    @return (n,n_samples) array where each entry is the average of a column\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "    \n",
    "    def extract_feature(img):\n",
    "        \"\"\"\n",
    "        @param img (m,n) array with values in (0,1)\n",
    "        @return (n,1) array where each entry is the average of a column\n",
    "        \"\"\"\n",
    "        m, n = img.shape\n",
    "        return np.sum(img, axis=0)/m\n",
    "    \n",
    "    for img in x:\n",
    "        arr.append(extract_feature(img))\n",
    "    return np.array(arr).T\n",
    "\n",
    "def top_bottom_features(x):\n",
    "    \"\"\"\n",
    "    This should either use or modify your code from the tutor questions.\n",
    "\n",
    "    @param x (n_samples,m,n) array with values in (0,1)\n",
    "    @return (2,n_samples) array where the first entry is the average of the\n",
    "    top half of the image = rows 0 to floor(m/2) [exclusive]\n",
    "    and the second entry is the average of the bottom half of the image\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "    \n",
    "    def extract_feature(img):\n",
    "        \"\"\"\n",
    "        @param img (m,n) array with values in (0,1)\n",
    "        @return (2,1) array where the first entry is the average of the\n",
    "        top half of the image = rows 0 to floor(m/2) [exclusive]\n",
    "        and the second entry is the average of the bottom half of the image\n",
    "        = rows floor(m/2) [inclusive] to m\n",
    "        \"\"\"\n",
    "        m, n = img.shape\n",
    "        row_ave = np.sum(img, axis=1)/n\n",
    "        final = np.array([0.0, 0.0])\n",
    "        final[0] = np.sum(row_ave[:m//2])/(m//2)\n",
    "        final[1] = np.sum(row_ave[m//2:])/(m-m//2)\n",
    "        return final\n",
    "    \n",
    "    for img in x:\n",
    "        arr.append(extract_feature(img))\n",
    "    return np.array(arr).T\n",
    "\n",
    "# use this function to evaluate accuracy\n",
    "#acc = hw3.get_classification_accuracy(raw_mnist_features(data), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 28, 28)\n",
      "X-val accuracy for raw classifying 0 and 1:  0.975\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# Analyze MNIST data\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Your code here to process the MNIST data\n",
    "\n",
    "d0 = mnist_data_all[0][\"images\"]\n",
    "d1 = mnist_data_all[1][\"images\"]\n",
    "y0 = np.repeat(-1, len(d0)).reshape(1,-1)\n",
    "y1 = np.repeat(1, len(d1)).reshape(1,-1)\n",
    "\n",
    "# data goes into the feature computation functions\n",
    "data_01 = np.vstack((d0, d1))\n",
    "# labels can directly go into the perceptron algorithm\n",
    "labels_01 = np.vstack((y0.T, y1.T)).T\n",
    "\n",
    "print(data_01.shape)\n",
    "\n",
    "print(\"X-val accuracy for raw classifying 0 and 1: \", hw3.xval_learning_alg(perceptron_50, raw_mnist_features(data_01), labels_01, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-val accuracy for raw classifying 2 and 4:  0.8641666666666665\n"
     ]
    }
   ],
   "source": [
    "d2 = mnist_data_all[2][\"images\"]\n",
    "d4 = mnist_data_all[4][\"images\"]\n",
    "y2 = np.repeat(-1, len(d2)).reshape(1,-1)\n",
    "y4 = np.repeat(1, len(d4)).reshape(1,-1)\n",
    "\n",
    "# data goes into the feature computation functions\n",
    "data_24 = np.vstack((d2, d4))\n",
    "# labels can directly go into the perceptron algorithm\n",
    "labels_24 = np.vstack((y2.T, y4.T)).T\n",
    "\n",
    "print(\"X-val accuracy for raw classifying 2 and 4: \", hw3.xval_learning_alg(perceptron_50, raw_mnist_features(data_24), labels_24, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-val accuracy for raw classifying 6 and 8:  0.9479166666666667\n"
     ]
    }
   ],
   "source": [
    "d6 = mnist_data_all[6][\"images\"]\n",
    "d8 = mnist_data_all[8][\"images\"]\n",
    "y6 = np.repeat(-1, len(d6)).reshape(1,-1)\n",
    "y8 = np.repeat(1, len(d8)).reshape(1,-1)\n",
    "\n",
    "# data goes into the feature computation functions\n",
    "data_68 = np.vstack((d6, d8))\n",
    "# labels can directly go into the perceptron algorithm\n",
    "labels_68 = np.vstack((y6.T, y8.T)).T\n",
    "\n",
    "print(\"X-val accuracy for raw classifying 6 and 8: \", hw3.xval_learning_alg(perceptron_50, raw_mnist_features(data_68), labels_68, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-val accuracy for classifying 0 and 9:  0.8183333333333334\n"
     ]
    }
   ],
   "source": [
    "d9 = mnist_data_all[9][\"images\"]\n",
    "y9 = np.repeat(1, len(d9)).reshape(1,-1)\n",
    "\n",
    "# data goes into the feature computation functions\n",
    "data_09 = np.vstack((d0, d9))\n",
    "# labels can directly go into the perceptron algorithm\n",
    "labels_09 = np.vstack((y0.T, y9.T)).T\n",
    "\n",
    "print(\"X-val accuracy for classifying 0 and 9: \", hw3.xval_learning_alg(perceptron_50, raw_mnist_features(data_09), labels_09, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 160)\n",
      "(160, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(top_bottom_features(data_01).shape)\n",
    "print(data_01.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-val accuracy for row classifying 0 and 1:  0.48125\n",
      "X-val accuracy for row classifying 2 and 4:  0.7754166666666668\n",
      "X-val accuracy for row classifying 6 and 8:  0.92125\n",
      "X-val accuracy for row classifying 9 and 0:  0.49083333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"X-val accuracy for row classifying 0 and 1: \", hw3.xval_learning_alg(perceptron_50, row_average_features(data_01), labels_01, 10))\n",
    "print(\"X-val accuracy for row classifying 2 and 4: \", hw3.xval_learning_alg(perceptron_50, row_average_features(data_24), labels_24, 10))\n",
    "print(\"X-val accuracy for row classifying 6 and 8: \", hw3.xval_learning_alg(perceptron_50, row_average_features(data_68), labels_68, 10))\n",
    "print(\"X-val accuracy for row classifying 9 and 0: \", hw3.xval_learning_alg(perceptron_50, row_average_features(data_09), labels_09, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-val accuracy for column classifying 0 and 1:  0.6375\n",
      "X-val accuracy for column classifying 2 and 4:  0.49749999999999994\n",
      "X-val accuracy for column classifying 6 and 8:  0.52125\n",
      "X-val accuracy for column classifying 9 and 0:  0.49083333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"X-val accuracy for column classifying 0 and 1: \", hw3.xval_learning_alg(perceptron_50, col_average_features(data_01), labels_01, 10))\n",
    "print(\"X-val accuracy for column classifying 2 and 4: \", hw3.xval_learning_alg(perceptron_50, col_average_features(data_24), labels_24, 10))\n",
    "print(\"X-val accuracy for column classifying 6 and 8: \", hw3.xval_learning_alg(perceptron_50, col_average_features(data_68), labels_68, 10))\n",
    "print(\"X-val accuracy for column classifying 9 and 0: \", hw3.xval_learning_alg(perceptron_50, col_average_features(data_09), labels_09, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-val accuracy for top-bottom classifying 0 and 1:  0.48125\n",
      "X-val accuracy for top-bottom classifying 2 and 4:  0.49749999999999994\n",
      "X-val accuracy for top-bottom classifying 6 and 8:  0.5650000000000001\n",
      "X-val accuracy for top-bottom classifying 9 and 0:  0.49083333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"X-val accuracy for top-bottom classifying 0 and 1: \", hw3.xval_learning_alg(perceptron_50, top_bottom_features(data_01), labels_01, 10))\n",
    "print(\"X-val accuracy for top-bottom classifying 2 and 4: \", hw3.xval_learning_alg(perceptron_50, top_bottom_features(data_24), labels_24, 10))\n",
    "print(\"X-val accuracy for top-bottom classifying 6 and 8: \", hw3.xval_learning_alg(perceptron_50, top_bottom_features(data_68), labels_68, 10))\n",
    "print(\"X-val accuracy for top-bottom classifying 9 and 0: \", hw3.xval_learning_alg(perceptron_50, top_bottom_features(data_09), labels_09, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
